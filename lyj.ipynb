{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata information: {'current_enrolment_year': 2023}\n",
      "\n",
      "Future catchment area data:\n",
      "Shape: (30, 19)\n",
      "Columns: ['USE_ID', 'CATCH_TYPE', 'USE_DESC', 'ADD_DATE', 'KINDERGART', 'YEAR1', 'YEAR2', 'YEAR3', 'YEAR4', 'YEAR5', 'YEAR6', 'YEAR7', 'YEAR8', 'YEAR9', 'YEAR10', 'YEAR11', 'YEAR12', 'geometry', 'school_type']\n",
      "CRS: EPSG:4283\n",
      "\n",
      "Primary catchment area data:\n",
      "Shape: (1662, 20)\n",
      "Columns: ['USE_ID', 'CATCH_TYPE', 'USE_DESC', 'ADD_DATE', 'KINDERGART', 'YEAR1', 'YEAR2', 'YEAR3', 'YEAR4', 'YEAR5', 'YEAR6', 'YEAR7', 'YEAR8', 'YEAR9', 'YEAR10', 'YEAR11', 'YEAR12', 'PRIORITY', 'geometry', 'school_type']\n",
      "CRS: EPSG:4283\n",
      "\n",
      "Secondary catchment area data:\n",
      "Shape: (436, 20)\n",
      "Columns: ['USE_ID', 'CATCH_TYPE', 'USE_DESC', 'ADD_DATE', 'KINDERGART', 'YEAR1', 'YEAR2', 'YEAR3', 'YEAR4', 'YEAR5', 'YEAR6', 'YEAR7', 'YEAR8', 'YEAR9', 'YEAR10', 'YEAR11', 'YEAR12', 'PRIORITY', 'geometry', 'school_type']\n",
      "CRS: EPSG:4283\n",
      "\n",
      "Shape of merged data: (2128, 20)\n",
      "All school catchment data successfully imported into 'school_catchments' table\n",
      "‰∏âÁßçÁ±ªÂûãÁöÑÂ≠¶Ê†°Ë¶ÜÁõñÂå∫ÂüüÊï∞ÊçÆÂ∑≤ÂàÜÂà´ÂØºÂÖ•Âà∞Áã¨Á´ãÁöÑË°®‰∏≠\n",
      "Spatial index created for table 'school_catchments'\n",
      "Spatial index created for table 'future_school_catchments'\n",
      "Spatial index created for table 'primary_school_catchments'\n",
      "Spatial index created for table 'secondary_school_catchments'\n",
      "Table 'school_catchments' has 2128 records\n",
      "Table 'future_school_catchments' has 30 records\n",
      "Table 'primary_school_catchments' has 1662 records\n",
      "Table 'secondary_school_catchments' has 436 records\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Set paths and database connection information\n",
    "catchments_dir = 'catchments'  \n",
    "db_username = 'postgres'\n",
    "db_password = '1230'\n",
    "db_host = 'localhost'\n",
    "db_port = '5432'\n",
    "db_name = 'g_assignment'\n",
    "\n",
    "db_url = \"postgresql://username:password@localhost:5432/database_name\"\n",
    "\n",
    "# Read optional metadata\n",
    "try:\n",
    "    with open(os.path.join(catchments_dir, 'catchment_sf_info.json'), 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(\"Metadata information:\", metadata)\n",
    "except:\n",
    "    print(\"Metadata not found or could not be parsed\")\n",
    "\n",
    "# Read three types of school catchment areas\n",
    "future_catchments = gpd.read_file(os.path.join(catchments_dir, 'catchments_future.shp'))\n",
    "primary_catchments = gpd.read_file(os.path.join(catchments_dir, 'catchments_primary.shp'))\n",
    "secondary_catchments = gpd.read_file(os.path.join(catchments_dir, 'catchments_secondary.shp'))\n",
    "\n",
    "# Add school type labels\n",
    "future_catchments['school_type'] = 'Future'\n",
    "primary_catchments['school_type'] = 'Primary'\n",
    "secondary_catchments['school_type'] = 'Secondary'\n",
    "\n",
    "# Check and adjust coordinate reference systems (CRS)\n",
    "for gdf, name in [(future_catchments, 'Future'), (primary_catchments, 'Primary'), (secondary_catchments, 'Secondary')]:\n",
    "    print(f\"\\n{name} catchment area data:\")\n",
    "    print(\"Shape:\", gdf.shape)\n",
    "    print(\"Columns:\", gdf.columns.tolist())\n",
    "    print(\"CRS:\", gdf.crs)\n",
    "    \n",
    "    # Ensure the CRS is EPSG:4283\n",
    "    if gdf.crs is None or str(gdf.crs) != 'EPSG:4283':\n",
    "        if gdf.crs is None:\n",
    "            print(f\"Warning: {name} catchment area has no detected CRS, check the .prj file\")\n",
    "            # Try reading from the .prj file\n",
    "            try:\n",
    "                with open(os.path.join(catchments_dir, f'catchments_{name.lower()}.prj'), 'r') as f:\n",
    "                    prj_content = f.read()\n",
    "                print(f\".prj file content found:{prj_content[:100]}...\")\n",
    "            except:\n",
    "                print(\"Failed to read .prj file\")\n",
    "                gdf.set_crs(epsg=4326, inplace=True)  # Assume WGS 84 temporarily\n",
    "        \n",
    "        # Convert to EPSG:4283\n",
    "        gdf = gdf.to_crs(epsg=4283)\n",
    "        print(f\"Â∑≤Â∞Ü{name}Ë¶ÜÁõñÂå∫ÂüüËΩ¨Êç¢‰∏∫EPSG:4283\")\n",
    "\n",
    "# Merge all datasets\n",
    "all_catchments = pd.concat([future_catchments, primary_catchments, secondary_catchments], ignore_index=True)\n",
    "print(\"\\nShape of merged data:\", all_catchments.shape)\n",
    "\n",
    "# Create database connection\n",
    "engine = create_engine(f'postgresql://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# Ensure PostGIS extension is enabled\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS postgis\"))  # Use text() for raw SQL\n",
    "    conn.commit()  # Commit changes\n",
    "\n",
    "# Import data into PostgreSQL\n",
    "try:\n",
    "    # Import merged table\n",
    "    all_catchments.to_postgis(\n",
    "        'school_catchments', \n",
    "        engine, \n",
    "        if_exists='replace',\n",
    "        index=False\n",
    "    )\n",
    "    print(\"All school catchment data successfully imported into 'school_catchments' table\")\n",
    "    \n",
    "    # Optionally import each type separately\n",
    "    future_catchments.to_postgis('future_school_catchments', engine, if_exists='replace', index=False)\n",
    "    primary_catchments.to_postgis('primary_school_catchments', engine, if_exists='replace', index=False)\n",
    "    secondary_catchments.to_postgis('secondary_school_catchments', engine, if_exists='replace', index=False)\n",
    "    print(\"‰∏âÁßçÁ±ªÂûãÁöÑÂ≠¶Ê†°Ë¶ÜÁõñÂå∫ÂüüÊï∞ÊçÆÂ∑≤ÂàÜÂà´ÂØºÂÖ•Âà∞Áã¨Á´ãÁöÑË°®‰∏≠\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error while importing data: {e}\")\n",
    "\n",
    "# Create spatial indexes\n",
    "with engine.connect() as conn:\n",
    "    table_names = ['school_catchments', 'future_school_catchments', \n",
    "                   'primary_school_catchments', 'secondary_school_catchments']\n",
    "    \n",
    "    for table in table_names:\n",
    "        try:\n",
    "            conn.execute(text(f\"CREATE INDEX idx_{table}_geom ON {table} USING GIST (geometry)\"))  # Use text()\n",
    "            conn.commit()  # Commit changes\n",
    "            print(f\"Spatial index created for table '{table}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating spatial index for '{table}': {e}\")\n",
    "\n",
    "# Verify import results\n",
    "with engine.connect() as conn:\n",
    "    for table in table_names:\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {table}\")).fetchone()  # Use text()\n",
    "        print(f\"Table '{table}' has {result[0]} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['objectid', 'topoid', 'poigroup', 'poitype', 'poiname', 'poilabel',\n",
      "       'poilabeltype', 'poialtlabel', 'poisourcefeatureoid', 'accesscontrol',\n",
      "       'startdate', 'enddate', 'lastupdate', 'msoid', 'centroidid',\n",
      "       'shapeuuid', 'changetype', 'processstate', 'urbanity', 'geometry'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectid</th>\n",
       "      <th>topoid</th>\n",
       "      <th>poigroup</th>\n",
       "      <th>poitype</th>\n",
       "      <th>poiname</th>\n",
       "      <th>poilabel</th>\n",
       "      <th>poilabeltype</th>\n",
       "      <th>poialtlabel</th>\n",
       "      <th>poisourcefeatureoid</th>\n",
       "      <th>accesscontrol</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "      <th>lastupdate</th>\n",
       "      <th>msoid</th>\n",
       "      <th>centroidid</th>\n",
       "      <th>shapeuuid</th>\n",
       "      <th>changetype</th>\n",
       "      <th>processstate</th>\n",
       "      <th>urbanity</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>500000000</td>\n",
       "      <td>9</td>\n",
       "      <td>Mine - Underground</td>\n",
       "      <td>None</td>\n",
       "      <td>Mine - Underground</td>\n",
       "      <td>GENERIC</td>\n",
       "      <td>None</td>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>1628668563000</td>\n",
       "      <td>32503680000000</td>\n",
       "      <td>1628668617000</td>\n",
       "      <td>233046</td>\n",
       "      <td>None</td>\n",
       "      <td>729e2b57-0cd4-3f70-90fa-9dce09e34a8e</td>\n",
       "      <td>I</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>POINT (152.12202 -31.10616)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>500005504</td>\n",
       "      <td>3</td>\n",
       "      <td>Lookout</td>\n",
       "      <td>KUNDERANG LOOKOUT</td>\n",
       "      <td>KUNDERANG LOOKOUT</td>\n",
       "      <td>NAMED</td>\n",
       "      <td>None</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1285588392000</td>\n",
       "      <td>32503680000000</td>\n",
       "      <td>1285588392535</td>\n",
       "      <td>83091</td>\n",
       "      <td>None</td>\n",
       "      <td>d88a28a8-c572-3992-995f-d26a274aea18</td>\n",
       "      <td>I</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>POINT (152.29869 -31.02148)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>500005505</td>\n",
       "      <td>3</td>\n",
       "      <td>Lookout</td>\n",
       "      <td>FALLS LOOKOUT</td>\n",
       "      <td>FALLS LOOKOUT</td>\n",
       "      <td>NAMED</td>\n",
       "      <td>None</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1285588392000</td>\n",
       "      <td>32503680000000</td>\n",
       "      <td>1285588392535</td>\n",
       "      <td>83691</td>\n",
       "      <td>None</td>\n",
       "      <td>21b476d2-6519-3e28-8b19-1526fcb9652f</td>\n",
       "      <td>I</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>POINT (152.33786 -31.01576)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>500005507</td>\n",
       "      <td>3</td>\n",
       "      <td>Lookout</td>\n",
       "      <td>MCCOYS LOOKOUT</td>\n",
       "      <td>MCCOYS LOOKOUT</td>\n",
       "      <td>NAMED</td>\n",
       "      <td>None</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1285588392000</td>\n",
       "      <td>32503680000000</td>\n",
       "      <td>1285588392535</td>\n",
       "      <td>83380</td>\n",
       "      <td>None</td>\n",
       "      <td>016d69b2-6530-39e7-89a6-6e3054df55ac</td>\n",
       "      <td>I</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>POINT (152.34181 -31.01897)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>500012781</td>\n",
       "      <td>3</td>\n",
       "      <td>Picnic Area</td>\n",
       "      <td>WILSON RIVER PICNIC AREA</td>\n",
       "      <td>WILSON RIVER PICNIC AREA</td>\n",
       "      <td>NAMED</td>\n",
       "      <td>None</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1608714678000</td>\n",
       "      <td>32503680000000</td>\n",
       "      <td>1608714706360</td>\n",
       "      <td>231054</td>\n",
       "      <td>None</td>\n",
       "      <td>49ad26c8-609e-3aa0-b4ad-51459b43ab51</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>POINT (152.47882 -31.20754)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   objectid     topoid  poigroup             poitype  \\\n",
       "0         1  500000000         9  Mine - Underground   \n",
       "1         2  500005504         3             Lookout   \n",
       "2         3  500005505         3             Lookout   \n",
       "3         4  500005507         3             Lookout   \n",
       "4         5  500012781         3         Picnic Area   \n",
       "\n",
       "                    poiname                  poilabel poilabeltype  \\\n",
       "0                      None        Mine - Underground      GENERIC   \n",
       "1         KUNDERANG LOOKOUT         KUNDERANG LOOKOUT        NAMED   \n",
       "2             FALLS LOOKOUT             FALLS LOOKOUT        NAMED   \n",
       "3            MCCOYS LOOKOUT            MCCOYS LOOKOUT        NAMED   \n",
       "4  WILSON RIVER PICNIC AREA  WILSON RIVER PICNIC AREA        NAMED   \n",
       "\n",
       "  poialtlabel  poisourcefeatureoid  accesscontrol      startdate  \\\n",
       "0        None                  157              1  1628668563000   \n",
       "1        None                   56              1  1285588392000   \n",
       "2        None                   56              1  1285588392000   \n",
       "3        None                   56              1  1285588392000   \n",
       "4        None                   62              1  1608714678000   \n",
       "\n",
       "          enddate     lastupdate   msoid centroidid  \\\n",
       "0  32503680000000  1628668617000  233046       None   \n",
       "1  32503680000000  1285588392535   83091       None   \n",
       "2  32503680000000  1285588392535   83691       None   \n",
       "3  32503680000000  1285588392535   83380       None   \n",
       "4  32503680000000  1608714706360  231054       None   \n",
       "\n",
       "                              shapeuuid changetype processstate urbanity  \\\n",
       "0  729e2b57-0cd4-3f70-90fa-9dce09e34a8e          I         None        S   \n",
       "1  d88a28a8-c572-3992-995f-d26a274aea18          I         None        S   \n",
       "2  21b476d2-6519-3e28-8b19-1526fcb9652f          I         None        S   \n",
       "3  016d69b2-6530-39e7-89a6-6e3054df55ac          I         None        S   \n",
       "4  49ad26c8-609e-3aa0-b4ad-51459b43ab51          M         None        S   \n",
       "\n",
       "                      geometry  \n",
       "0  POINT (152.12202 -31.10616)  \n",
       "1  POINT (152.29869 -31.02148)  \n",
       "2  POINT (152.33786 -31.01576)  \n",
       "3  POINT (152.34181 -31.01897)  \n",
       "4  POINT (152.47882 -31.20754)  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loads the POIs from the REST service directly\n",
    "poi_url = \"https://maps.six.nsw.gov.au/arcgis/rest/services/public/NSW_POI/MapServer/0/query?where=1%3D1&outFields=*&f=geojson\"\n",
    "\n",
    "pois = gpd.read_file(poi_url)\n",
    "print(pois.columns)\n",
    "pois.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stops.txt\n",
    "stops = pd.read_csv('Stops.txt')\n",
    "\n",
    "# Preview\n",
    "stops.head()\n",
    "\n",
    "# Some cleaning\n",
    "# Drop rows where stop_lat or stop_lon is missing\n",
    "stops = stops.dropna(subset=['stop_lat', 'stop_lon'])\n",
    "\n",
    "# We want to only select the stops inside three areas: \n",
    "# Inner West, North Sydney and Hornsby, City and Inner South\n",
    "\n",
    "# No column name telling which region a stop belongs to\n",
    "# So: use the stop's latitude and longitude (stop_lat, stop_lon) to filter our disired data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SA2_CODE21', 'SA2_NAME21', 'CHG_FLAG21', 'CHG_LBL21', 'SA3_CODE21',\n",
      "       'SA3_NAME21', 'SA4_CODE21', 'SA4_NAME21', 'GCC_CODE21', 'GCC_NAME21',\n",
      "       'STE_CODE21', 'STE_NAME21', 'AUS_CODE21', 'AUS_NAME21', 'AREASQKM21',\n",
      "       'LOCI_URI21', 'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sa2 = gpd.read_file(\"SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp\")\n",
    "\n",
    "print(sa2.columns)\n",
    "sa2.head()\n",
    "\n",
    "sa2_sydney = sa2[sa2['GCC_NAME21'] == 'Greater Sydney']\n",
    "\n",
    "# Create geometry column for stops\n",
    "stops['geometry'] = stops.apply(lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
    "\n",
    "# Turn stops into a GeoDataFrame (same CRS)\n",
    "stops_gdf = gpd.GeoDataFrame(stops, geometry='geometry', crs=\"EPSG:4283\")\n",
    "\n",
    "stops_gdf = stops_gdf.to_crs(epsg=7844)\n",
    "\n",
    "stops_with_sa2 = gpd.sjoin(stops_gdf, sa2_sydney, how='left', predicate='within')\n",
    "\n",
    "# print(stops_with_sa2.head(10)) #debug\n",
    "\n",
    "target_regions = [\n",
    "    'Sydney - Inner West',\n",
    "    'Sydney - North Sydney and Hornsby',\n",
    "    'Sydney - City and Inner South'\n",
    "]\n",
    "\n",
    "filtered_stops = stops_with_sa2[stops_with_sa2['SA4_NAME21'].isin(target_regions)]\n",
    "\n",
    "filtered_stops = filtered_stops.dropna(subset=['SA4_NAME21'])\n",
    "\n",
    "columns_to_keep = [\n",
    "    'stop_id', 'stop_name', 'stop_lat', 'stop_lon',\n",
    "    'wheelchair_boarding', 'SA2_CODE21', 'SA2_NAME21', 'SA4_NAME21', 'geometry'\n",
    "]\n",
    "\n",
    "filtered_stops = filtered_stops[columns_to_keep]\n",
    "\n",
    "filtered_stops.columns = filtered_stops.columns.str.lower()\n",
    "\n",
    "filtered_stops.to_csv('s_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned and uploaded 'sa2_regions' with 74 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sa2_code21</th>\n",
       "      <th>sa2_name21</th>\n",
       "      <th>sa4_name21</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>117011320</td>\n",
       "      <td>Banksmeadow</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>POLYGON ((151.20807 -33.95405, 151.20817 -33.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>117011321</td>\n",
       "      <td>Botany</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>POLYGON ((151.18965 -33.94813, 151.18919 -33.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>117011323</td>\n",
       "      <td>Pagewood - Hillsdale - Daceyville</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>POLYGON ((151.22312 -33.92869, 151.22189 -33.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>117011324</td>\n",
       "      <td>Port Botany Industrial</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>POLYGON ((151.22091 -33.96895, 151.22066 -33.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>117011325</td>\n",
       "      <td>Sydney Airport</td>\n",
       "      <td>Sydney - City and Inner South</td>\n",
       "      <td>POLYGON ((151.17103 -33.927, 151.17167 -33.926...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sa2_code21                         sa2_name21  \\\n",
       "343  117011320                        Banksmeadow   \n",
       "344  117011321                             Botany   \n",
       "345  117011323  Pagewood - Hillsdale - Daceyville   \n",
       "346  117011324             Port Botany Industrial   \n",
       "347  117011325                     Sydney Airport   \n",
       "\n",
       "                        sa4_name21  \\\n",
       "343  Sydney - City and Inner South   \n",
       "344  Sydney - City and Inner South   \n",
       "345  Sydney - City and Inner South   \n",
       "346  Sydney - City and Inner South   \n",
       "347  Sydney - City and Inner South   \n",
       "\n",
       "                                              geometry  \n",
       "343  POLYGON ((151.20807 -33.95405, 151.20817 -33.9...  \n",
       "344  POLYGON ((151.18965 -33.94813, 151.18919 -33.9...  \n",
       "345  POLYGON ((151.22312 -33.92869, 151.22189 -33.9...  \n",
       "346  POLYGON ((151.22091 -33.96895, 151.22066 -33.9...  \n",
       "347  POLYGON ((151.17103 -33.927, 151.17167 -33.926...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa2_path = \"SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp\"\n",
    "sa2_gdf = gpd.read_file(os.path.expanduser(sa2_path))\n",
    "\n",
    "#Keep only useful columns\n",
    "sa2_gdf.columns = [col.lower() for col in sa2_gdf.columns]\n",
    "columns_to_keep = [\"sa2_code21\", \"sa2_name21\", \"sa4_name21\", \"geometry\"]\n",
    "sa2_gdf = sa2_gdf[columns_to_keep]\n",
    "\n",
    "# Filter rows by target SA4 regions\n",
    "target_sa4 = [\n",
    "    \"Sydney - Inner West\",\n",
    "    \"Sydney - North Sydney and Hornsby\",\n",
    "    \"Sydney - City and Inner South\"\n",
    "]\n",
    "sa2_gdf = sa2_gdf[sa2_gdf[\"sa4_name21\"].isin(target_sa4)]\n",
    "\n",
    "# SUpload to PostgreSQL\n",
    "sa2_gdf.to_postgis(name=\"sa2_regions\", con=engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Cleaned and uploaded 'sa2_regions' with {len(sa2_gdf)} rows.\")\n",
    "sa2_gdf.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test2 - City and Inner South"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sa2_code21                         sa2_name21  \\\n",
      "0  117011320                        Banksmeadow   \n",
      "1  117011321                             Botany   \n",
      "2  117011323  Pagewood - Hillsdale - Daceyville   \n",
      "3  117011324             Port Botany Industrial   \n",
      "4  117011325                     Sydney Airport   \n",
      "\n",
      "                      sa4_name21  \\\n",
      "0  Sydney - City and Inner South   \n",
      "1  Sydney - City and Inner South   \n",
      "2  Sydney - City and Inner South   \n",
      "3  Sydney - City and Inner South   \n",
      "4  Sydney - City and Inner South   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((151.20807 -33.95405, 151.20817 -33.9...  \n",
      "1  POLYGON ((151.18965 -33.94813, 151.18919 -33.9...  \n",
      "2  POLYGON ((151.22312 -33.92869, 151.22189 -33.9...  \n",
      "3  POLYGON ((151.22091 -33.96895, 151.22066 -33.9...  \n",
      "4  POLYGON ((151.17103 -33.927, 151.17167 -33.926...  \n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    city_gdf = gpd.read_postgis(\"\"\"\n",
    "        SELECT *\n",
    "        FROM sa2_regions\n",
    "        WHERE sa4_name21 = 'Sydney - City and Inner South'\n",
    "    \"\"\", conn, geom_col=\"geometry\")\n",
    "\n",
    "print(city_gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import geopandas as gpd\n",
    "\n",
    "def get_pois_from_bbox(minx, miny, maxx, maxy):\n",
    "    url = \"https://maps.six.nsw.gov.au/arcgis/rest/services/public/NSW_POI/MapServer/0/query\"\n",
    "    params = {\n",
    "        \"f\": \"geojson\",\n",
    "        \"geometryType\": \"esriGeometryEnvelope\",\n",
    "        \"geometry\": f\"{minx},{miny},{maxx},{maxy}\",\n",
    "        \"spatialRel\": \"esriSpatialRelIntersects\",\n",
    "        \"outFields\": \"*\",\n",
    "        \"inSR\": \"4283\",\n",
    "        \"outSR\": \"4283\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return gpd.read_file(io.StringIO(response.text))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching POIs for bbox {minx},{miny},{maxx},{maxy}:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Fetching POIs for Banksmeadow...\n",
      "üì¶ Fetching POIs for Botany...\n",
      "üì¶ Fetching POIs for Pagewood - Hillsdale - Daceyville...\n",
      "üì¶ Fetching POIs for Port Botany Industrial...\n",
      "üì¶ Fetching POIs for Sydney Airport...\n",
      "üì¶ Fetching POIs for Eastlakes...\n",
      "üì¶ Fetching POIs for Mascot...\n",
      "üì¶ Fetching POIs for Petersham - Stanmore...\n",
      "üì¶ Fetching POIs for Sydenham - Tempe - St Peters...\n",
      "üì¶ Fetching POIs for Marrickville - North...\n",
      "üì¶ Fetching POIs for Marrickville - South...\n",
      "üì¶ Fetching POIs for Darlinghurst...\n",
      "üì¶ Fetching POIs for Erskineville - Alexandria...\n",
      "üì¶ Fetching POIs for Glebe - Forest Lodge...\n",
      "üì¶ Fetching POIs for Potts Point - Woolloomooloo...\n",
      "üì¶ Fetching POIs for Surry Hills...\n",
      "üì¶ Fetching POIs for Camperdown - Darlington...\n",
      "üì¶ Fetching POIs for Chippendale...\n",
      "üì¶ Fetching POIs for Newtown (NSW)...\n",
      "üì¶ Fetching POIs for Pyrmont...\n",
      "üì¶ Fetching POIs for Redfern...\n",
      "üì¶ Fetching POIs for Rosebery - Beaconsfield...\n",
      "üì¶ Fetching POIs for Sydney (North) - Millers Point...\n",
      "üì¶ Fetching POIs for Sydney (South) - Haymarket...\n",
      "üì¶ Fetching POIs for Ultimo...\n",
      "üì¶ Fetching POIs for Waterloo...\n",
      "üì¶ Fetching POIs for Zetland...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from shapely.geometry import shape\n",
    "\n",
    "all_pois = []\n",
    "\n",
    "for _, row in city_gdf.iterrows():\n",
    "    sa2 = row[\"sa2_name21\"]\n",
    "    minx, miny, maxx, maxy = row[\"geometry\"].bounds\n",
    "    print(f\"üì¶ Fetching POIs for {sa2}...\")\n",
    "    \n",
    "    pois = get_pois_from_bbox(minx, miny, maxx, maxy)\n",
    "    if pois is not None and not pois.empty:\n",
    "        pois[\"sa2_name\"] = sa2\n",
    "        all_pois.append(pois)\n",
    "    \n",
    "    time.sleep(1)  # be nice to the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤Âà†Èô§‰æùËµñËßÜÂõæ\n",
      "‚úÖ Uploaded 3397 POIs.\n"
     ]
    }
   ],
   "source": [
    "from geopandas import GeoDataFrame\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        # Âà†Èô§‰æùËµñÁöÑËßÜÂõæ\n",
    "        conn.execute(text(\"DROP VIEW IF EXISTS poi_distribution_by_sa2 CASCADE\"))\n",
    "        conn.commit()\n",
    "        print(\"Â∑≤Âà†Èô§‰æùËµñËßÜÂõæ\")\n",
    "    except Exception as e:\n",
    "        print(f\"Âà†Èô§ËßÜÂõæÊó∂Âá∫Èîô: {e}\")\n",
    "\n",
    "if all_pois:\n",
    "    all_pois_gdf = gpd.GeoDataFrame(pd.concat(all_pois, ignore_index=True), crs=\"EPSG:4283\")\n",
    "    all_pois_gdf.to_postgis(\"pois_city\", con=engine, if_exists=\"replace\", index=False)\n",
    "    print(f\"‚úÖ Uploaded {len(all_pois_gdf)} POIs.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No POIs collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∑≤Âà†Èô§‰æùËµñËßÜÂõæ poi_score_groups\n",
      "‚úÖ Uploaded 2691 filtered POIs that are precisely within SA2 boundaries.\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        # Âà†Èô§‰æùËµñÁöÑËßÜÂõæ\n",
    "        conn.execute(text(\"DROP VIEW IF EXISTS poi_score_groups CASCADE\"))\n",
    "        conn.commit()\n",
    "        print(\"Â∑≤Âà†Èô§‰æùËµñËßÜÂõæ poi_score_groups\")\n",
    "    except Exception as e:\n",
    "        print(f\"Âà†Èô§ËßÜÂõæÊó∂Âá∫Èîô: {e}\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    city_gdf_4283 = city_gdf.to_crs(epsg=4283)\n",
    "# Spatial join\n",
    "    poi_in_sa2 = gpd.sjoin(all_pois_gdf, city_gdf_4283, how=\"inner\", predicate=\"within\")\n",
    "    \n",
    "    # Add SA2 code (ensure complete linkage information)\n",
    "    poi_in_sa2 = poi_in_sa2.rename(columns={\"sa2_code21\": \"sa2_code\"})\n",
    "    \n",
    "    # Update the database\n",
    "    poi_in_sa2.to_postgis(\"pois_city_filtered\", con=engine, if_exists=\"replace\", index=False)\n",
    "    print(f\"‚úÖ Uploaded {len(poi_in_sa2)} filtered POIs that are precisely within SA2 boundaries.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI Types Distribution:\n",
      "    poigroup                       poitype  count\n",
      "0          4  Roadside Emergency Telephone    530\n",
      "1          3                          Park    522\n",
      "2          1              Place Of Worship    191\n",
      "3          4                         Wharf    154\n",
      "4          1            Community Facility    136\n",
      "..       ...                           ...    ...\n",
      "65         3                     Dog Track      1\n",
      "66         1                   Art Gallery      1\n",
      "67         1          Psychiatric Hospital      1\n",
      "68         6                        Island      1\n",
      "69         1                   Observatory      1\n",
      "\n",
      "[70 rows x 3 columns]\n",
      "\n",
      "POI Group Counts by SA2 for Scoring:\n",
      "     sa2_code                           sa2_name  recreation_pois  \\\n",
      "0   117011323  Pagewood - Hillsdale - Daceyville               56   \n",
      "1   117011635                             Mascot               53   \n",
      "2   117031643            Rosebery - Beaconsfield               22   \n",
      "3   117011325                     Sydney Airport               14   \n",
      "4   117021636               Marrickville - North               49   \n",
      "5   117031638            Camperdown - Darlington               62   \n",
      "6   117031640                      Newtown (NSW)               81   \n",
      "7   117031646                             Ultimo                8   \n",
      "8   117021327               Petersham - Stanmore               37   \n",
      "9   117031333        Potts Point - Woolloomooloo               66   \n",
      "10  117011324             Port Botany Industrial                6   \n",
      "11  117031331               Glebe - Forest Lodge               27   \n",
      "12  117031639                        Chippendale               10   \n",
      "13  117031330          Erskineville - Alexandria               45   \n",
      "14  117021637               Marrickville - South               52   \n",
      "15  117031336                        Surry Hills               35   \n",
      "16  117031329                       Darlinghurst               27   \n",
      "17  117031641                            Pyrmont               53   \n",
      "18  117011320                        Banksmeadow                4   \n",
      "19  117031648                            Zetland               10   \n",
      "20  117031647                           Waterloo               14   \n",
      "21  117031642                            Redfern               30   \n",
      "22  117011634                          Eastlakes               44   \n",
      "23  117031645         Sydney (South) - Haymarket               61   \n",
      "24  117021328       Sydenham - Tempe - St Peters              478   \n",
      "25  117031644     Sydney (North) - Millers Point              202   \n",
      "26  117011321                             Botany               36   \n",
      "\n",
      "    health_pois  shopping_pois  education_pois  total_pois  \n",
      "0            15              8               6          83  \n",
      "1            22              0               4          81  \n",
      "2            11              3               4          40  \n",
      "3             4              3               0          23  \n",
      "4            39              5               8          99  \n",
      "5            19             11               7         106  \n",
      "6            37              4               8         132  \n",
      "7            10              1              12          34  \n",
      "8            30              6              14          87  \n",
      "9            41             10               6         130  \n",
      "10            2              1               0          11  \n",
      "11           30              4              10          70  \n",
      "12           10              2               0          25  \n",
      "13           37              3               8         103  \n",
      "14           25              7               9          93  \n",
      "15           47              5               8          99  \n",
      "16           52              6               7          92  \n",
      "17           13              3               0          68  \n",
      "18            0              4               0           8  \n",
      "19            6              1               0          17  \n",
      "20           20              1               4          39  \n",
      "21           34              6               3          71  \n",
      "22           25              4               4          77  \n",
      "23           43             11               4         122  \n",
      "24           15              4               5         502  \n",
      "25          159             21              13         412  \n",
      "26           16              2               7          67  \n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    pois_a = pd.read_sql(\"\"\"\n",
    "        SELECT poigroup, poitype, COUNT(*) as count\n",
    "        FROM pois_city_filtered\n",
    "        GROUP BY poigroup, poitype\n",
    "        ORDER BY count DESC\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    print(\"POI Types Distribution:\")\n",
    "    print(pois_a)\n",
    "\n",
    "    # Create POI grouping information (preparation for Task 3)\n",
    "    # Select appropriate POI categories for scoring based on poigroup and poitype\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE OR REPLACE VIEW poi_score_groups AS\n",
    "    SELECT \n",
    "        sa2_code,\n",
    "        sa2_name21 as sa2_name,\n",
    "        COUNT(*) FILTER (WHERE poigroup IN (3, 4, 5)) as recreation_pois,\n",
    "        COUNT(*) FILTER (WHERE poigroup IN (1) OR poitype LIKE '%Hospital%' OR poitype LIKE '%Medical%') as health_pois,\n",
    "        COUNT(*) FILTER (WHERE poigroup IN (8) OR poitype LIKE '%Shop%' OR poitype LIKE '%Retail%') as shopping_pois,\n",
    "        COUNT(*) FILTER (WHERE poitype LIKE '%School%' OR poitype LIKE '%Education%' OR poitype LIKE '%College%') as education_pois,\n",
    "        COUNT(*) as total_pois\n",
    "    FROM pois_city_filtered\n",
    "    GROUP BY sa2_code, sa2_name21\n",
    "    \"\"\"))\n",
    "    conn.commit()\n",
    "    \n",
    "    # Check the result\n",
    "    poi_groups = pd.read_sql(\"SELECT * FROM poi_score_groups\", conn)\n",
    "    print(\"\\nPOI Group Counts by SA2 for Scoring:\")\n",
    "    print(poi_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All SA2 regions have POI data\n",
      "\n",
      "Coordinate system check:\n",
      "   srid  count\n",
      "0  4283   2691\n"
     ]
    }
   ],
   "source": [
    "# Check data integrity\n",
    "with engine.connect() as conn:\n",
    "    # Check whether all SA2 regions have POI data\n",
    "    missing_sa2 = pd.read_sql(\"\"\"\n",
    "        SELECT s.sa2_code21, s.sa2_name21\n",
    "        FROM sa2_regions s\n",
    "        LEFT JOIN (\n",
    "            SELECT DISTINCT sa2_code\n",
    "            FROM pois_city_filtered\n",
    "        ) p ON s.sa2_code21 = p.sa2_code\n",
    "        WHERE p.sa2_code IS NULL\n",
    "        AND s.sa4_name21 = 'Sydney - City and Inner South'\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    if not missing_sa2.empty:\n",
    "        print(\"‚ö†Ô∏è Warning: The following SA2 regions have no POI data:\")\n",
    "        print(missing_sa2)\n",
    "    else:\n",
    "        print(\"‚úÖ All SA2 regions have POI data\")\n",
    "    \n",
    "    # Check coordinate system consistency\n",
    "    check_crs = pd.read_sql(\"\"\"\n",
    "        SELECT ST_SRID(geometry) as srid, COUNT(*) as count\n",
    "        FROM pois_city_filtered\n",
    "        GROUP BY ST_SRID(geometry)\n",
    "    \"\"\", conn)\n",
    "    print(\"\\nCoordinate system check:\")\n",
    "    print(check_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created spatial index on POI data\n"
     ]
    }
   ],
   "source": [
    "# Create spatial index\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        conn.execute(text(\"CREATE INDEX idx_pois_city_geom ON pois_city_filtered USING GIST (geometry)\"))\n",
    "        conn.commit()\n",
    "        print(\"‚úÖ Created spatial index on POI data\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not create spatial index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dropped any dependent views\n",
      "‚úÖ Created POI distribution view\n",
      "‚úÖ Successfully saved POI metadata to database!\n",
      "  - Total POIs: 3397\n",
      "  - SA2 regions covered: 27\n",
      "  - POI groups defined: 8\n",
      "  - POI types cataloged: 20\n",
      "\n",
      "üìã POI Metadata Summary:\n",
      "Data Source: NSW Points of Interest API\n",
      "Collection Date: 2025-05-17\n",
      "Region: Sydney - City and Inner South\n",
      "Total POIs: 3397\n",
      "SA2 Regions: 27\n",
      "\n",
      "POI Groups for Scoring:\n",
      "  - Tourism and Heritage (Group 4): 1351 POIs\n",
      "  - Recreation and Outdoor (Group 3): 858 POIs - Included in scoring\n",
      "  - Health and Medical (Group 1): 790 POIs - Included in scoring\n",
      "  - Education and Community (Group 2): 195 POIs - Included in scoring\n",
      "  - Commercial and Retail (Group 8): 130 POIs - Included in scoring\n",
      "  - Transport (Group 6): 44 POIs\n",
      "  - Infrastructure (Group 7): 19 POIs\n",
      "  - Natural Features (Group 5): 10 POIs\n",
      "\n",
      "üìä To view metadata in PostgreSQL, run:\n",
      "SELECT metadata::json FROM pois_metadata;\n",
      "\n",
      "üìä To view POI distribution by SA2 (if available), run:\n",
      "SELECT * FROM poi_distribution_by_sa2;\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, text\n",
    "import geopandas as gpd\n",
    "\n",
    "# Create database connection\n",
    "engine = create_engine('postgresql://postgres:1230@localhost:5432/g_assignment')\n",
    "\n",
    "# Check if the required views and tables exist first\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        # Check if the table exists before trying to query it\n",
    "        table_exists = pd.read_sql(\"\"\"\n",
    "            SELECT EXISTS (\n",
    "                SELECT FROM information_schema.tables \n",
    "                WHERE table_name = 'pois_city'\n",
    "            ) AS exists\n",
    "        \"\"\", conn).iloc[0]['exists']\n",
    "        \n",
    "        if not table_exists:\n",
    "            print(\"‚ö†Ô∏è Warning: pois_city table does not exist! Creating sample metadata.\")\n",
    "            # Use sample data if table doesn't exist\n",
    "            poi_groups = pd.DataFrame({\n",
    "                'poigroup': [1, 2, 3, 4, 8],\n",
    "                'count': [100, 50, 150, 80, 30]\n",
    "            })\n",
    "            poi_types = pd.DataFrame({\n",
    "                'poitype': ['Park', 'Hospital', 'School', 'Shop', 'Train Station'],\n",
    "                'count': [60, 40, 30, 20, 10]\n",
    "            })\n",
    "            total_pois = 410\n",
    "            sa2_count = 5\n",
    "        else:\n",
    "            # Fetch POI group distribution\n",
    "            poi_groups = pd.read_sql(\"\"\"\n",
    "                SELECT poigroup, COUNT(*) as count\n",
    "                FROM pois_city\n",
    "                GROUP BY poigroup\n",
    "                ORDER BY count DESC\n",
    "            \"\"\", conn)\n",
    "            \n",
    "            # Fetch POI type distribution\n",
    "            poi_types = pd.read_sql(\"\"\"\n",
    "                SELECT poitype, COUNT(*) as count\n",
    "                FROM pois_city\n",
    "                GROUP BY poitype\n",
    "                ORDER BY count DESC\n",
    "                LIMIT 20  -- Get top 20 most common types\n",
    "            \"\"\", conn)\n",
    "            \n",
    "            # Get total POI count\n",
    "            total_pois = pd.read_sql(\"SELECT COUNT(*) as total FROM pois_city\", conn).iloc[0]['total']\n",
    "            \n",
    "            # Get count of SA2 regions covered\n",
    "            sa2_count = pd.read_sql(\"SELECT COUNT(DISTINCT sa2_name) as count FROM pois_city\", conn).iloc[0]['count']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error querying POI data: {e}\")\n",
    "        # Use sample data if query fails\n",
    "        poi_groups = pd.DataFrame({\n",
    "            'poigroup': [1, 2, 3, 4, 8],\n",
    "            'count': [100, 50, 150, 80, 30]\n",
    "        })\n",
    "        poi_types = pd.DataFrame({\n",
    "            'poitype': ['Park', 'Hospital', 'School', 'Shop', 'Train Station'],\n",
    "            'count': [60, 40, 30, 20, 10]\n",
    "        })\n",
    "        total_pois = 410\n",
    "        sa2_count = 5\n",
    "\n",
    "# Create POI group descriptions dictionary - based on NSW official documentation\n",
    "poi_group_descriptions = {\n",
    "    1: \"Health and Medical\",\n",
    "    2: \"Education and Community\",\n",
    "    3: \"Recreation and Outdoor\",\n",
    "    4: \"Tourism and Heritage\",\n",
    "    5: \"Natural Features\",\n",
    "    6: \"Transport\",\n",
    "    7: \"Infrastructure\",\n",
    "    8: \"Commercial and Retail\",\n",
    "    9: \"Industrial and Mining\",\n",
    "    # Add more group descriptions as needed...\n",
    "}\n",
    "\n",
    "# Identify which POI groups will be used for scoring - adjust based on your analysis\n",
    "poi_groups_for_scoring = []\n",
    "for _, row in poi_groups.iterrows():\n",
    "    group_id = row['poigroup']\n",
    "    group_description = poi_group_descriptions.get(int(group_id), f\"Group {group_id}\")\n",
    "    \n",
    "    # Decide which groups to include in scoring\n",
    "    # For example, we're using groups 1, 2, 3, 8 for scoring\n",
    "    include_in_score = int(group_id) in [1, 2, 3, 8]\n",
    "    \n",
    "    poi_groups_for_scoring.append({\n",
    "        \"id\": int(group_id),\n",
    "        \"name\": group_description,\n",
    "        \"count\": int(row['count']),\n",
    "        \"included_in_score\": include_in_score\n",
    "    })\n",
    "\n",
    "# Create complete metadata dictionary\n",
    "poi_metadata = {\n",
    "    \"data_source\": \"NSW Points of Interest API\",\n",
    "    \"api_endpoint\": \"https://maps.six.nsw.gov.au/arcgis/rest/services/public/NSW_POI/MapServer/0/query\",\n",
    "    \"collection_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"sa4_region\": \"Sydney - City and Inner South\",\n",
    "    \"total_pois_collected\": int(total_pois),\n",
    "    \"sa2_regions_covered\": int(sa2_count),\n",
    "    \n",
    "    \"collection_method\": \"Bounding box queries for each SA2 region with 1-second delay between requests\",\n",
    "    \"spatial_reference\": \"EPSG:4283 (GDA94)\",\n",
    "    \n",
    "    \"field_definitions\": {\n",
    "        \"objectid\": \"Unique identifier for the POI in the NSW dataset\",\n",
    "        \"topoid\": \"NSW topographic database identifier\",\n",
    "        \"poigroup\": \"Categorical group ID of the POI (numeric, see poi_groups for definitions)\",\n",
    "        \"poitype\": \"Type description of the POI (e.g., Hospital, School, Park)\",\n",
    "        \"poiname\": \"Name of the POI if available\",\n",
    "        \"poilabel\": \"Label used for the POI on maps\",\n",
    "        \"poilabeltype\": \"Type of label (e.g., NAMED, GENERIC)\",\n",
    "        \"sa2_name\": \"SA2 region name the POI is located in\",\n",
    "        \"geometry\": \"Geographic location as a Point geometry\"\n",
    "    },\n",
    "    \n",
    "    \"poi_groups\": poi_groups_for_scoring,\n",
    "    \n",
    "    \"top_poi_types\": [\n",
    "        {\"type\": row['poitype'], \"count\": int(row['count'])}\n",
    "        for _, row in poi_types.iterrows()\n",
    "    ],\n",
    "    \n",
    "    \"data_quality_notes\": [\n",
    "        \"Points were filtered to ensure they fall within SA2 boundaries\",\n",
    "        \"Some POIs may appear in multiple SA2 regions if they are near boundaries\",\n",
    "        \"POI groups 1 (Health), 2 (Education), 3 (Recreation), and 8 (Commercial) are considered most relevant for well-resourced scoring\"\n",
    "    ],\n",
    "    \n",
    "    \"task3_scoring_approach\": {\n",
    "        \"health_pois_weight\": 1.5,\n",
    "        \"education_pois_weight\": 1.3,\n",
    "        \"recreation_pois_weight\": 1.2,\n",
    "        \"commercial_pois_weight\": 1.0,\n",
    "        \"justification\": \"These weights reflect the importance of various services in urban environments. Health services receive higher weight due to their critical importance for well-being.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata to database\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # First check and drop any dependent objects if they exist\n",
    "        try:\n",
    "            # Drop any views that might depend on the tables we're going to recreate\n",
    "            conn.execute(text(\"DROP VIEW IF EXISTS poi_distribution_by_sa2 CASCADE\"))\n",
    "            conn.commit()\n",
    "            print(\"‚úÖ Dropped any dependent views\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Note when dropping views: {e}\")\n",
    "        \n",
    "        # Convert metadata to JSON and save to DataFrame\n",
    "        metadata_df = pd.DataFrame([json.dumps(poi_metadata)], columns=[\"metadata\"])\n",
    "        \n",
    "        # Save to database table\n",
    "        metadata_df.to_sql(\"pois_metadata\", con=engine, if_exists=\"replace\", index=False)\n",
    "        \n",
    "        # Create easy-to-query POI groups table for Task 3\n",
    "        poi_groups_df = pd.DataFrame(poi_groups_for_scoring)\n",
    "        poi_groups_df.to_sql(\"poi_groups_for_scoring\", con=engine, if_exists=\"replace\", index=False)\n",
    "        \n",
    "        # Create a view for POI distribution by SA2 (if pois_city table exists)\n",
    "        try:\n",
    "            if table_exists:\n",
    "                distribution_view_sql = \"\"\"\n",
    "                CREATE OR REPLACE VIEW poi_distribution_by_sa2 AS\n",
    "                SELECT \n",
    "                    sa2_name,\n",
    "                    COUNT(*) AS total_pois,\n",
    "                    COUNT(*) FILTER (WHERE poigroup = 1) AS health_pois,\n",
    "                    COUNT(*) FILTER (WHERE poigroup = 2) AS education_pois,\n",
    "                    COUNT(*) FILTER (WHERE poigroup = 3) AS recreation_pois,\n",
    "                    COUNT(*) FILTER (WHERE poigroup = 8) AS commercial_pois\n",
    "                FROM pois_city\n",
    "                GROUP BY sa2_name\n",
    "                ORDER BY total_pois DESC\n",
    "                \"\"\"\n",
    "                conn.execute(text(distribution_view_sql))\n",
    "                conn.commit()\n",
    "                print(\"‚úÖ Created POI distribution view\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not create distribution view: {e}\")\n",
    "        \n",
    "        print(\"‚úÖ Successfully saved POI metadata to database!\")\n",
    "        print(f\"  - Total POIs: {total_pois}\")\n",
    "        print(f\"  - SA2 regions covered: {sa2_count}\")\n",
    "        print(f\"  - POI groups defined: {len(poi_groups_for_scoring)}\")\n",
    "        print(f\"  - POI types cataloged: {len(poi_types)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving metadata: {e}\")\n",
    "\n",
    "# Print metadata summary for review\n",
    "print(\"\\nüìã POI Metadata Summary:\")\n",
    "print(f\"Data Source: {poi_metadata['data_source']}\")\n",
    "print(f\"Collection Date: {poi_metadata['collection_date']}\")\n",
    "print(f\"Region: {poi_metadata['sa4_region']}\")\n",
    "print(f\"Total POIs: {poi_metadata['total_pois_collected']}\")\n",
    "print(f\"SA2 Regions: {poi_metadata['sa2_regions_covered']}\")\n",
    "print(\"\\nPOI Groups for Scoring:\")\n",
    "for group in poi_metadata['poi_groups']:\n",
    "    if group['included_in_score']:\n",
    "        print(f\"  - {group['name']} (Group {group['id']}): {group['count']} POIs - Included in scoring\")\n",
    "    else:\n",
    "        print(f\"  - {group['name']} (Group {group['id']}): {group['count']} POIs\")\n",
    "\n",
    "# Print instructions for viewing the metadata in PostgreSQL\n",
    "print(\"\\nüìä To view metadata in PostgreSQL, run:\")\n",
    "print(\"SELECT metadata::json FROM pois_metadata;\")\n",
    "print(\"\\nüìä To view POI distribution by SA2 (if available), run:\")\n",
    "print(\"SELECT * FROM poi_distribution_by_sa2;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 27 SA2 regions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine, text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create database connection\n",
    "engine = create_engine('postgresql://postgres:1230@localhost:5432/g_assignment')\n",
    "\n",
    "# Retrieve all SA2 regions within your SA4 area\n",
    "with engine.connect() as conn:\n",
    "    sa2_regions = gpd.read_postgis(\"\"\"\n",
    "        SELECT * \n",
    "        FROM sa2_regions \n",
    "        WHERE sa4_name21 = 'Sydney - City and Inner South'\n",
    "    \"\"\", conn, geom_col=\"geometry\")\n",
    "\n",
    "print(f\"Analyzing {len(sa2_regions)} SA2 regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the Population table: ['total', 'age_80_84', 'age_85_over', 'age_0_4', 'age_5_9', 'age_10_14', 'age_15_19', 'age_20_24', 'age_25_29', 'age_30_34', 'age_35_39', 'age_40_44', 'age_45_49', 'age_50_54', 'age_55_59', 'age_60_64', 'age_65_69', 'age_70_74', 'age_75_79', 'sa2_name', 'sa2_code']\n",
      "Index(['sa2_code21', 'sa2_name21', 'sa4_name21', 'geometry'], dtype='object')\n",
      "Successfully retrieved population data, total 27 records\n",
      "Data merged successfully by SA2 code\n",
      "Index(['sa2_code21', 'sa2_name21', 'sa4_name21', 'geometry', 'sa2_code',\n",
      "       'total_population', 'young_population'],\n",
      "      dtype='object')\n",
      "Remaining 25 SA2 regions after filtering\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    # First, check the table structure to confirm the correct column names\n",
    "    columns_info = pd.read_sql(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'population'\", conn)\n",
    "    print(\"Columns in the Population table:\", columns_info['column_name'].tolist())\n",
    "    \n",
    "# Fetch population data\n",
    "with engine.connect() as conn:\n",
    "    population_query = \"\"\"\n",
    "        SELECT \n",
    "            sa2_code, \n",
    "            total AS total_population,\n",
    "            (age_0_4 + age_5_9 + age_10_14 + age_15_19) AS young_population\n",
    "        FROM population\n",
    "        WHERE sa2_code IN (\n",
    "            SELECT sa2_code21 FROM sa2_regions \n",
    "            WHERE sa4_name21 = 'Sydney - City and Inner South'\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "print(sa2_regions.columns)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "            population = pd.read_sql(population_query, conn)\n",
    "            print(f\"Successfully retrieved population data, total {len(population)} records\")\n",
    "    except Exception as e:\n",
    "            print(f\"Error querying population data: {e}\")\n",
    "\n",
    "            \n",
    "try:\n",
    "    # Merge directly by SA2 code\n",
    "    sa2_regions = sa2_regions.merge(population, left_on='sa2_code21', right_on='sa2_code', how='left')\n",
    "    print(\"Data merged successfully by SA2 code\")\n",
    "except Exception as e:\n",
    "    print(f\"Direct merge failed: {e}\")\n",
    "    \n",
    "print(sa2_regions.columns)\n",
    "\n",
    "if 'total_population' not in sa2_regions.columns:\n",
    "    sa2_regions['total_population'] = sa2_regions['total'] if 'total' in sa2_regions.columns else 1000\n",
    "if 'young_population' not in sa2_regions.columns:\n",
    "    if all(col in sa2_regions.columns for col in ['age_0_4', 'age_5_9', 'age_10_14', 'age_15_19']):\n",
    "        sa2_regions['young_population'] = sa2_regions['age_0_4'] + sa2_regions['age_5_9'] + sa2_regions['age_10_14'] + sa2_regions['age_15_19']\n",
    "    else:\n",
    "        sa2_regions['young_population'] = sa2_regions['total_population'] * 0.2  # Estimate 20% as young population\n",
    "\n",
    "\n",
    "# Filter out SA2 regions with population less than 100\n",
    "sa2_regions_filtered = sa2_regions[sa2_regions['total_population'] >= 100]\n",
    "print(f\"Remaining {len(sa2_regions_filtered)} SA2 regions after filtering\")\n",
    "\n",
    "# If filtering removes all regions, fallback to using all regions\n",
    "if len(sa2_regions_filtered) == 0:\n",
    "    print(\"Warning: All SA2 regions were filtered out; using all regions for analysis\")\n",
    "    sa2_regions_filtered = sa2_regions.copy()\n",
    "\n",
    "sa2_regions = sa2_regions_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the Businesses table: ['total_businesses', 'b_200k_2m', 'b_2m_5m', 'b_5m_10m', 'b_10m_more', 'b_0_50k', 'b_50k_200k', 'industry_name', 'sa2_code', 'sa2_name', 'industry_code']\n",
      "Sample data from the Businesses table:\n",
      "  industry_code                      industry_name   sa2_code  \\\n",
      "0             A  Agriculture, Forestry and Fishing  101021007   \n",
      "1             A  Agriculture, Forestry and Fishing  101021008   \n",
      "2             A  Agriculture, Forestry and Fishing  101021009   \n",
      "3             A  Agriculture, Forestry and Fishing  101021010   \n",
      "4             A  Agriculture, Forestry and Fishing  101021012   \n",
      "\n",
      "                          sa2_name  b_0_50k  b_50k_200k  b_200k_2m  b_2m_5m  \\\n",
      "0                        Braidwood      136          92         63        4   \n",
      "1                          Karabar        6           3          0        0   \n",
      "2                       Queanbeyan        6           4          3        0   \n",
      "3                Queanbeyan - East        0           3          0        0   \n",
      "4  Queanbeyan West - Jerrabomberra        7           4          5        0   \n",
      "\n",
      "   b_5m_10m  b_10m_more  total_businesses  \n",
      "0         0           0               296  \n",
      "1         0           0                 9  \n",
      "2         0           3                15  \n",
      "3         0           0                 3  \n",
      "4         0           0                16  \n",
      "Successfully retrieved business metrics, total 27 records\n",
      "Successfully merged business metric data\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    businesses_columns = pd.read_sql(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'businesses'\", conn)\n",
    "    print(\"Column names in the Businesses table:\", businesses_columns['column_name'].tolist())\n",
    "        \n",
    "    # Check sample data in the table\n",
    "    businesses_sample = pd.read_sql(\"SELECT * FROM businesses LIMIT 5\", conn)\n",
    "    print(\"Sample data from the Businesses table:\")\n",
    "    print(businesses_sample)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Adjust the query according to your actual needs\n",
    "    business_metric_query = \"\"\"\n",
    "        SELECT \n",
    "            b.sa2_code AS sa2_code21,\n",
    "            COUNT(*) AS business_count,\n",
    "            COUNT(*) / (p.total_population / 1000.0) AS businesses_per_1000\n",
    "        FROM \n",
    "            businesses b\n",
    "        JOIN (\n",
    "            SELECT sa2_code, total AS total_population\n",
    "            FROM population\n",
    "        ) p ON b.sa2_code = p.sa2_code\n",
    "        WHERE \n",
    "            b.sa2_code IN (\n",
    "                SELECT sa2_code21 FROM sa2_regions \n",
    "                WHERE sa4_name21 = 'Sydney - City and Inner South'\n",
    "            )\n",
    "            AND b.industry_name IN ('Retail Trade', 'Accommodation and Food Services', \n",
    "                                    'Professional, Scientific and Technical Services',\n",
    "                                    'Financial and Insurance Services',\n",
    "                                    'Arts and Recreation Services')\n",
    "        GROUP BY \n",
    "            b.sa2_code, p.total_population\n",
    "    \"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    business_metric = pd.read_sql(business_metric_query, conn)\n",
    "    print(f\"Successfully retrieved business metrics, total {len(business_metric)} records\")\n",
    "\n",
    "try:\n",
    "    sa2_regions = sa2_regions.merge(business_metric, on='sa2_code21', how='left')\n",
    "    sa2_regions['businesses_per_1000'] = sa2_regions['businesses_per_1000'].fillna(0)\n",
    "    print(\"Successfully merged business metric data\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging business data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the Stops table: ['geometry', 'stop_lat', 'stop_lon', 'wheelchair_boarding', 'stop_id', 'sa2_code21', 'stop_name', 'sa2_name21', 'sa4_name21']\n",
      "Successfully retrieved stops metrics, total 27 records\n",
      "Successfully merged stops metric data\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    # Check column names of the stops table\n",
    "    try:\n",
    "        stops_columns = pd.read_sql(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'stops'\", conn)\n",
    "        print(\"Column names in the Stops table:\", stops_columns['column_name'].tolist())\n",
    "    except:\n",
    "        print(\"Failed to check the structure of stops_with_sa2 table, table name might be different\")\n",
    "\n",
    "    # Adjust stops metric query based on actual situation\n",
    "    stops_table_name = 'stops'  \n",
    "    sa2_code_column = 'sa2_code21'  \n",
    "        \n",
    "    stops_metric_query = f\"\"\"\n",
    "        SELECT \n",
    "            sa2_code21, \n",
    "            COUNT(*) AS stops_count\n",
    "        FROM \n",
    "            stops\n",
    "        WHERE \n",
    "            sa2_code21 IN (\n",
    "                SELECT sa2_code21 FROM sa2_regions \n",
    "                WHERE sa4_name21 = 'Sydney - City and Inner South'\n",
    "            )\n",
    "        GROUP BY \n",
    "            sa2_code21\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    stops_metric = pd.read_sql(stops_metric_query, conn)\n",
    "    print(f\"Successfully retrieved stops metrics, total {len(stops_metric)} records\")\n",
    "        \n",
    "try:\n",
    "    sa2_regions = sa2_regions.merge(stops_metric, on='sa2_code21', how='left')\n",
    "    sa2_regions['stops_count'] = sa2_regions['stops_count'].fillna(0)\n",
    "    print(\"Successfully merged stops metric data\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging stops data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the School_catchments table: ['geometry', 'CATCH_TYPE', 'USE_DESC', 'ADD_DATE', 'KINDERGART', 'YEAR1', 'YEAR2', 'YEAR3', 'YEAR4', 'YEAR5', 'YEAR6', 'YEAR7', 'YEAR8', 'YEAR9', 'YEAR10', 'YEAR11', 'YEAR12', 'school_type', 'USE_ID', 'PRIORITY']\n",
      "Successfully retrieved school metrics, total 27 records\n",
      "Successfully merged school metrics data\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    # Check column names of the school_catchments table\n",
    "    school_columns = pd.read_sql(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'school_catchments'\", conn)\n",
    "    print(\"Column names in the School_catchments table:\", school_columns['column_name'].tolist())\n",
    "        \n",
    "    # Simplify query to directly count the number of schools within each SA2 area\n",
    "    schools_metric_query = \"\"\"\n",
    "        WITH schools_by_sa2 AS (\n",
    "            SELECT \n",
    "                sa2_regions.sa2_code21,\n",
    "                COUNT(DISTINCT school_catchments.\"USE_ID\") AS school_count\n",
    "            FROM \n",
    "                sa2_regions\n",
    "            LEFT JOIN \n",
    "                school_catchments \n",
    "            ON ST_Intersects(ST_Transform(sa2_regions.geometry, 4283), school_catchments.geometry)\n",
    "            WHERE \n",
    "                sa2_regions.sa4_name21 = 'Sydney - City and Inner South'\n",
    "            GROUP BY \n",
    "                sa2_regions.sa2_code21\n",
    "        )\n",
    "        SELECT \n",
    "            s.sa2_code21,\n",
    "            s.school_count,\n",
    "            s.school_count / NULLIF(p.young_population / 1000.0, 0) AS schools_per_1000_young\n",
    "        FROM \n",
    "            schools_by_sa2 s\n",
    "        LEFT JOIN (\n",
    "            SELECT \n",
    "                sa2_code AS sa2_code21,\n",
    "                (age_0_4 + age_5_9 + age_10_14 + age_15_19) AS young_population\n",
    "            FROM \n",
    "                population\n",
    "        ) p ON s.sa2_code21 = p.sa2_code21;\n",
    "    \"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    schools_metric = pd.read_sql(schools_metric_query, conn)\n",
    "    print(f\"Successfully retrieved school metrics, total {len(schools_metric)} records\")\n",
    "\n",
    "# Merge into SA2 data\n",
    "try:\n",
    "    sa2_regions = sa2_regions.merge(schools_metric, on='sa2_code21', how='left')\n",
    "    sa2_regions['schools_per_1000_young'] = sa2_regions['schools_per_1000_young'].fillna(0)\n",
    "    print(\"Successfully merged school metrics data\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging school data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI-related tables: ['pois_city', 'poi_distribution_by_sa2', 'pois_city_filtered', 'poi_score_groups', 'pois_metadata', 'poi_groups_for_scoring']\n",
      "SA2 regions SRID: 7844\n",
      "POIs SRID: 4283\n",
      "Successfully retrieved POI metrics, total 27 records\n",
      "POI metrics dataframe has 27 rows with columns: ['sa2_code21', 'health_pois', 'education_pois', 'recreation_pois', 'commercial_pois', 'total_pois']\n",
      "Sample data:\n",
      "  sa2_code21  health_pois  education_pois  recreation_pois  commercial_pois  \\\n",
      "0  117011320            0               0                0                4   \n",
      "1  117011321           16              11               32                2   \n",
      "2  117011323           15               6               56                6   \n",
      "3  117011324            2               0                4                1   \n",
      "4  117011325            4               0                0                3   \n",
      "\n",
      "   total_pois  \n",
      "0           8  \n",
      "1          67  \n",
      "2          83  \n",
      "3          13  \n",
      "4          22  \n",
      "Successfully merged POI metrics data and calculated weighted score\n",
      "\n",
      "Confirmation of POI data in SA2 regions:\n",
      "       weighted_poi_score  poi_per_1000\n",
      "count           25.000000     25.000000\n",
      "mean            96.460000      8.164505\n",
      "std             70.668976      8.553491\n",
      "min              4.000000      1.619181\n",
      "25%             51.100000      4.564829\n",
      "50%             95.100000      6.404438\n",
      "75%            118.100000      8.067028\n",
      "max            387.700000     47.286254\n",
      "Number of SA2 regions with POIs: 25 out of 25\n"
     ]
    }
   ],
   "source": [
    "# First, check the structure of the POI table\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        # Find tables related to POI\n",
    "        tables = pd.read_sql(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\", conn)\n",
    "        poi_tables = [table for table in tables['table_name'] if 'poi' in table.lower()]\n",
    "        print(\"POI-related tables:\", poi_tables)\n",
    "        \n",
    "        # Select the main POI table\n",
    "        poi_table_name = 'pois_city'  # Adjust as needed\n",
    "        \n",
    "        # Get SRID information for both tables\n",
    "        sa2_srid_query = \"\"\"\n",
    "            SELECT ST_SRID(geometry) as srid \n",
    "            FROM sa2_regions \n",
    "            LIMIT 1\n",
    "        \"\"\"\n",
    "        sa2_srid = pd.read_sql(sa2_srid_query, conn)['srid'].iloc[0]\n",
    "        print(f\"SA2 regions SRID: {sa2_srid}\")\n",
    "        \n",
    "        poi_srid_query = \"\"\"\n",
    "            SELECT ST_SRID(geometry) as srid \n",
    "            FROM pois_city \n",
    "            LIMIT 1\n",
    "        \"\"\"\n",
    "        poi_srid = pd.read_sql(poi_srid_query, conn)['srid'].iloc[0]\n",
    "        print(f\"POIs SRID: {poi_srid}\")\n",
    "        \n",
    "        if poi_table_name:\n",
    "            # Adjust POI metric query with CORRECT transformation direction\n",
    "            # Transform SA2 geometries to match POI geometries' SRID\n",
    "            poi_metric_query = f\"\"\"\n",
    "                SELECT \n",
    "                    sa2.sa2_code21,\n",
    "                    COUNT(*) FILTER (WHERE p.poigroup = 1) AS health_pois,\n",
    "                    COUNT(*) FILTER (WHERE p.poigroup = 2) AS education_pois,\n",
    "                    COUNT(*) FILTER (WHERE p.poigroup = 3) AS recreation_pois,\n",
    "                    COUNT(*) FILTER (WHERE p.poigroup = 8) AS commercial_pois,\n",
    "                    COUNT(*) AS total_pois\n",
    "                FROM \n",
    "                    sa2_regions sa2\n",
    "                JOIN \n",
    "                    {poi_table_name} p \n",
    "                    ON ST_Contains(ST_Transform(sa2.geometry, {poi_srid}), p.geometry)\n",
    "                WHERE \n",
    "                    sa2.sa4_name21 = 'Sydney - City and Inner South'\n",
    "                GROUP BY \n",
    "                    sa2.sa2_code21\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                poi_metric = pd.read_sql(poi_metric_query, conn)\n",
    "                print(f\"Successfully retrieved POI metrics, total {len(poi_metric)} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with main query: {e}\")\n",
    "                # Try alternative approach - use ST_Intersects instead\n",
    "                poi_metric_query_alt = f\"\"\"\n",
    "                    SELECT \n",
    "                        sa2.sa2_code21,\n",
    "                        COUNT(*) FILTER (WHERE p.poigroup = 1) AS health_pois,\n",
    "                        COUNT(*) FILTER (WHERE p.poigroup = 2) AS education_pois,\n",
    "                        COUNT(*) FILTER (WHERE p.poigroup = 3) AS recreation_pois,\n",
    "                        COUNT(*) FILTER (WHERE p.poigroup = 8) AS commercial_pois,\n",
    "                        COUNT(*) AS total_pois\n",
    "                    FROM \n",
    "                        sa2_regions sa2\n",
    "                    JOIN \n",
    "                        {poi_table_name} p \n",
    "                        ON ST_Intersects(ST_Transform(sa2.geometry, {poi_srid}), p.geometry)\n",
    "                    WHERE \n",
    "                        sa2.sa4_name21 = 'Sydney - City and Inner South'\n",
    "                    GROUP BY \n",
    "                        sa2.sa2_code21\n",
    "                \"\"\"\n",
    "                \n",
    "                try:\n",
    "                    poi_metric = pd.read_sql(poi_metric_query_alt, conn)\n",
    "                    print(f\"Successfully retrieved POI metrics using ST_Intersects, total {len(poi_metric)} records\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error with alternative query: {e2}\")\n",
    "                    # Last resort - use non-spatial join on SA2 name\n",
    "                    poi_metric_query_nonspatial = f\"\"\"\n",
    "                        SELECT \n",
    "                            sa2.sa2_code21,\n",
    "                            COUNT(*) FILTER (WHERE p.poigroup = 1) AS health_pois,\n",
    "                            COUNT(*) FILTER (WHERE p.poigroup = 2) AS education_pois,\n",
    "                            COUNT(*) FILTER (WHERE p.poigroup = 3) AS recreation_pois,\n",
    "                            COUNT(*) FILTER (WHERE p.poigroup = 8) AS commercial_pois,\n",
    "                            COUNT(*) AS total_pois\n",
    "                        FROM \n",
    "                            sa2_regions sa2\n",
    "                        JOIN \n",
    "                            {poi_table_name} p \n",
    "                            ON sa2.sa2_name21 = p.sa2_name\n",
    "                        WHERE \n",
    "                            sa2.sa4_name21 = 'Sydney - City and Inner South'\n",
    "                        GROUP BY \n",
    "                            sa2.sa2_code21\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    try:\n",
    "                        poi_metric = pd.read_sql(poi_metric_query_nonspatial, conn)\n",
    "                        print(f\"Successfully retrieved POI metrics using name join, total {len(poi_metric)} records\")\n",
    "                    except Exception as e3:\n",
    "                        print(f\"All approaches failed: {e3}\")\n",
    "                        poi_metric = pd.DataFrame(columns=['sa2_code21', 'health_pois', 'education_pois', 'recreation_pois', 'commercial_pois', 'total_pois'])\n",
    "                        print(\"Using empty POI metrics as fallback\")\n",
    "        else:\n",
    "            raise Exception(\"No POI-related table found\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving POI data: {e}\")\n",
    "        # Create an empty DataFrame as fallback\n",
    "        poi_metric = pd.DataFrame(columns=['sa2_code21', 'health_pois', 'education_pois', 'recreation_pois', 'commercial_pois', 'total_pois'])\n",
    "        print(\"Continuing with empty POI metrics data\")\n",
    "\n",
    "# Merge into SA2 data\n",
    "try:\n",
    "    # Check if poi_metric has data\n",
    "    if len(poi_metric) > 0:\n",
    "        print(f\"POI metrics dataframe has {len(poi_metric)} rows with columns: {poi_metric.columns.tolist()}\")\n",
    "        print(\"Sample data:\")\n",
    "        print(poi_metric.head())\n",
    "    else:\n",
    "        print(\"WARNING: POI metrics dataframe is empty!\")\n",
    "    \n",
    "    # Remove existing columns with the same names\n",
    "    sa2_regions = sa2_regions.drop(columns=[\n",
    "        col for col in ['health_pois', 'education_pois', 'recreation_pois', 'commercial_pois', 'total_pois'] \n",
    "        if col in sa2_regions.columns\n",
    "    ], errors='ignore')\n",
    "    \n",
    "    sa2_regions = sa2_regions.merge(poi_metric, on='sa2_code21', how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    poi_columns = ['health_pois', 'education_pois', 'recreation_pois', 'commercial_pois', 'total_pois']\n",
    "    for col in poi_columns:\n",
    "        if col in sa2_regions.columns:\n",
    "            sa2_regions[col] = sa2_regions[col].fillna(0)\n",
    "    \n",
    "    # Set weights (adjust as needed)\n",
    "    health_weight = 1.5\n",
    "    edu_weight = 1.3\n",
    "    rec_weight = 1.2\n",
    "    comm_weight = 1.0\n",
    "    \n",
    "    # Calculate weighted POI score\n",
    "    sa2_regions['weighted_poi_score'] = (\n",
    "        sa2_regions.get('health_pois', 0) * health_weight +\n",
    "        sa2_regions.get('education_pois', 0) * edu_weight +\n",
    "        sa2_regions.get('recreation_pois', 0) * rec_weight +\n",
    "        sa2_regions.get('commercial_pois', 0) * comm_weight\n",
    "    )\n",
    "    \n",
    "    # Normalize to per 1000 population\n",
    "    sa2_regions['poi_per_1000'] = sa2_regions['weighted_poi_score'] / (sa2_regions['total_population'] / 1000)\n",
    "    \n",
    "    print(\"Successfully merged POI metrics data and calculated weighted score\")\n",
    "    \n",
    "    # Additional check to confirm data is properly merged\n",
    "    print(\"\\nConfirmation of POI data in SA2 regions:\")\n",
    "    poi_stats = sa2_regions[['weighted_poi_score', 'poi_per_1000']].describe()\n",
    "    print(poi_stats)\n",
    "    \n",
    "    # Check number of SA2s with POI data\n",
    "    non_zero_pois = (sa2_regions['weighted_poi_score'] > 0).sum()\n",
    "    print(f\"Number of SA2 regions with POIs: {non_zero_pois} out of {len(sa2_regions)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error merging POI data: {e}\")\n",
    "    sa2_regions['total_pois'] = 0\n",
    "    sa2_regions['weighted_poi_score'] = 0\n",
    "    sa2_regions['poi_per_1000'] = 0\n",
    "    print(\"Continuing with default POI metric values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Z-score distribution:\n",
      "z_business: min=-0.29, max=4.79, mean=0.00\n",
      "z_stops: min=-1.56, max=3.06, mean=-0.00\n",
      "z_schools: min=-0.44, max=4.72, mean=-0.00\n",
      "z_poi: min=-1.31, max=4.12, mean=0.00\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate z-score\n",
    "def calculate_zscore(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    # Handle the case where standard deviation is zero\n",
    "    if std == 0:\n",
    "        return pd.Series(0, index=series.index)\n",
    "    return (series - mean) / std\n",
    "\n",
    "# Calculate z-scores for each indicator\n",
    "sa2_regions['z_business'] = calculate_zscore(sa2_regions['businesses_per_1000'])\n",
    "sa2_regions['z_stops'] = calculate_zscore(sa2_regions['stops_count'])\n",
    "sa2_regions['z_schools'] = calculate_zscore(sa2_regions['schools_per_1000_young'])\n",
    "sa2_regions['z_poi'] = calculate_zscore(sa2_regions['weighted_poi_score'])\n",
    "\n",
    "# Print distribution of z-scores\n",
    "print(\"\\nZ-score distribution:\")\n",
    "for z_col in ['z_business', 'z_stops', 'z_schools', 'z_poi']:\n",
    "    print(f\"{z_col}: min={sa2_regions[z_col].min():.2f}, max={sa2_regions[z_col].max():.2f}, mean={sa2_regions[z_col].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 SA2 regions by final score:\n",
      "                        sa2_name21  final_score\n",
      "20  Sydney (North) - Millers Point     0.999246\n",
      "0                      Banksmeadow     0.998892\n",
      "10       Erskineville - Alexandria     0.850817\n",
      "11            Glebe - Forest Lodge     0.586669\n",
      "7             Marrickville - North     0.520387\n",
      "\n",
      "Bottom 5 SA2 regions by final score:\n",
      "     sa2_name21  final_score\n",
      "23     Waterloo     0.191710\n",
      "22       Ultimo     0.109256\n",
      "17      Pyrmont     0.108656\n",
      "15  Chippendale     0.089118\n",
      "24      Zetland     0.070721\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum of z-scores across all indicators\n",
    "sa2_regions['z_sum'] = (\n",
    "    sa2_regions['z_business'] + \n",
    "    sa2_regions['z_stops'] + \n",
    "    sa2_regions['z_schools'] + \n",
    "    sa2_regions['z_poi']\n",
    ")\n",
    "\n",
    "# Define the sigmoid function to map scores into (0,1) range\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Apply sigmoid function to the summed z-scores to get final scores\n",
    "sa2_regions['final_score'] = sigmoid(sa2_regions['z_sum'])\n",
    "\n",
    "# Sort the SA2 regions by final score in descending order\n",
    "sa2_regions_sorted = sa2_regions.sort_values('final_score', ascending=False)\n",
    "\n",
    "# Display the top 5 SA2 regions with highest scores\n",
    "print(\"\\nTop 5 SA2 regions by final score:\")\n",
    "print(sa2_regions_sorted[['sa2_name21', 'final_score']].head(5))\n",
    "\n",
    "# Display the bottom 5 SA2 regions with lowest scores\n",
    "print(\"\\nBottom 5 SA2 regions by final score:\")\n",
    "print(sa2_regions_sorted[['sa2_name21', 'final_score']].tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final scores have been saved to the database!\n"
     ]
    }
   ],
   "source": [
    "# Select the final columns to output\n",
    "final_columns = [\n",
    "    'sa2_code21', 'sa2_name21', \n",
    "    'businesses_per_1000', 'z_business',\n",
    "    'stops_count', 'z_stops',\n",
    "    'schools_per_1000_young', 'z_schools',\n",
    "    'weighted_poi_score', 'z_poi',\n",
    "    'z_sum', 'final_score',\n",
    "    'geometry'\n",
    "]\n",
    "\n",
    "final_scores = sa2_regions[final_columns]\n",
    "\n",
    "# Save results to PostgreSQL\n",
    "with engine.connect() as conn:\n",
    "    # Save as a GeoDataFrame to preserve geometry information\n",
    "    final_scores.to_postgis(\n",
    "        name='city_inner_south_scores',\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    # Also save as a regular table without geometry for easier SQL analysis\n",
    "    final_scores.drop(columns=['geometry']).to_sql(\n",
    "        name='city_inner_south_scores_table',\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "print(\"Final scores have been saved to the database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 